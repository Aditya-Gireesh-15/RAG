# -*- coding: utf-8 -*-
"""Multimodal_RAG_model_mistral7BV3_churn_data_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ui0g7MGtQqJ2yZOotRLPe0c8Iflv7Csb
"""

!pip install -r Full_test_requirements.txt

"""Importing Library"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import faiss
import os
import wikipedia
import fitz
import nltk
import shutil
import re

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from langchain_community.llms import HuggingFaceEndpoint
from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEndpoint
from google.colab import userdata, files
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

import os
import cv2
import easyocr

"""Loading pdf Data"""

folder_name = 'pdfs'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)

uploaded = files.upload()
for filename in uploaded.keys():
    shutil.move(filename, os.path.join(folder_name, filename))

"""Loading Image data and retriving Text using OCR"""

image_folder = '/content/image_folder'  # Update this path to your folder containing images
threshold = 0.25
max_words_per_chunk = 100  # Adjust as needed

reader = easyocr.Reader(['en'], gpu=False)

# === Function to Chunk Text ===
def chunk_text(text, max_words=100):
    words = text.split()
    return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

all_text_chunks_img = []

for image_file in os.listdir(image_folder):
    if not image_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
        continue

    image_path = os.path.join(image_folder, image_file)
    img = cv2.imread(image_path)

    text_detect = reader.readtext(img)
    detected_text = []

    print(f"\nProcessing image: {image_file}")
    print("Detected text blocks:")

    for t in text_detect:
        bbox, text, score = t
        if score > threshold:
            detected_text.append(text)
            print(f" - {text} (score: {score:.2f})")
            # Optional visualization
            bbox = [tuple(map(int, point)) for point in bbox]
            cv2.rectangle(img, bbox[0], bbox[2], (0, 255, 0), 2)
            cv2.putText(img, text, bbox[0], cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

    full_text = ' '.join(detected_text)
    chunks_img = chunk_text(full_text, max_words=max_words_per_chunk)

    for chunk in chunks_img:
        all_text_chunks_img.append({
            'image_file': image_file,
            'chunk_text': chunk
        })

    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(f"Annotated: {image_file}")
    plt.axis('off')
    plt.show()

# === Final Output: List of Chunks ===
print("\nAll OCR Chunks (ready for RAG):\n")
for item in all_text_chunks_img:
    print(f"[{item['image_file']}] {item['chunk_text']}\n")

"""Loading CSV data"""

from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/customer_churn.csv'
df = pd.read_csv(file_path)
df.head()

"""Data Cleaning"""

df.replace(r'^\s*$', np.nan, regex=True, inplace=True)
df.columns

df.isnull().sum()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.fillna(df['TotalCharges'].mean(), inplace=True)

df=df[['customerID','gender','SeniorCitizen','Partner','tenure','InternetService','OnlineSecurity','MonthlyCharges', 'TotalCharges','Contract', 'Churn'] ]
df.head(2)

df.isnull().sum()

df = df[(df['tenure'] > 0)]
df.shape

df.groupby(['Churn', 'gender','Contract']).size()

df5 = df[['gender','Contract','Churn']]
df_churn_count = df5.groupby(['Churn', 'gender','Contract']).size().reset_index(name='Count')
df_churn_count

plt.figure(figsize=(10,9))
sb.barplot(data=df_churn_count, x='Contract', y='Count', hue='Churn', ci=None,
            palette={'Yes': 'red', 'No': 'green'}, dodge=True)

plt.xlabel("Contract")
plt.ylabel("Count")
plt.title("Churn Contract Status")
plt.legend(title="Churn")

plt.show()

df.TotalCharges.describe()

Q1 = df['TotalCharges'].quantile(0.25)
Q3 = df['TotalCharges'].quantile(0.75)
IQR = Q3 - Q1  #Interquartile Range

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

#Remove_outliers
df_cleaned = df[(df['TotalCharges'] >= lower_bound) & (df['TotalCharges'] <= upper_bound)]

print(f"Original size: {df.shape[0]}, After outlier removal: {df_cleaned.shape[0]}")

plt.figure(figsize=(8, 6))
sb.boxplot(data=df_cleaned, y='TotalCharges', color='skyblue')
plt.title("Box Plot of TotalCharges (Outliers Removed)")
plt.ylabel("TotalCharges")
plt.show()

df_MonthlyCharges=df[['Contract', 'InternetService' , 'MonthlyCharges']]
mean_monthly_charges = df_MonthlyCharges.groupby('Contract')['MonthlyCharges'].mean()
print(mean_monthly_charges)

ISP_mean_monthly_charges = df_MonthlyCharges.groupby('InternetService')['MonthlyCharges'].mean()
print(ISP_mean_monthly_charges)

columns_df = ", ".join(df.columns)
columns_df

df["SeniorCitizen"]= df["SeniorCitizen"].map({0: "No", 1: "Yes"})
df.head()

df.shape

df.info()

df['tenure'] = df['tenure'].astype('int32')
df['MonthlyCharges'] = df['MonthlyCharges'].astype('float32')
df['TotalCharges'] = df['TotalCharges'].astype('float32')

"""Creating Text csv chunks"""

grouped = df.groupby("customerID")
chunks_csv = []
metadata = []

for name, group in grouped:

    text_chunk = f"customerID: {name}\n"
    for _, row in group.iterrows():
        entry = f"  - customerID: {row['customerID']}, gender: {row['gender']}, SeniorCitizen: {row['SeniorCitizen']}, Partner: {row['Partner']}, tenure: {row['tenure']} , InternetService: {row['InternetService']} , OnlineSecurity: {row['OnlineSecurity']} , MonthlyCharges: {row['MonthlyCharges']}, TotalCharges: {row['TotalCharges']}, Contract: {row['Contract']}, Churn: {row['Churn']}"
        text_chunk += entry + "\n"

    chunks_csv.append(text_chunk)
    metadata.append({"group": name})

"""*Loading* Wikepedia and creating chunks"""

wiki_topics = ["Churn rate"]
wiki_chunks = []
wiki_metadata = []

for topic in wiki_topics:
    try:
        content = wikipedia.page(topic).content
        chunks = [content[i:i+512] for i in range(0, len(content), 512)]
        wiki_chunks.extend(chunks)
        wiki_metadata.extend([{"source": "wikipedia", "topic": topic}] * len(chunks))
    except wikipedia.exceptions.DisambiguationError as e:
        print(f"Disambiguation required for: {topic}, options: {e.options}")
    except wikipedia.exceptions.PageError:
        print(f"Page not found: {topic}")

"""Creating PDF chunks"""

pdf_folder = "pdfs"
pdf_chunks = []
pdf_metadata = []

for file_name in os.listdir(pdf_folder):
    if file_name.endswith(".pdf"):
        file_path = os.path.join(pdf_folder, file_name)
        doc = fitz.open(file_path)
        for page in doc:
            text = page.get_text()
            chunks = [text[i:i+512] for i in range(0, len(text), 512)]
            pdf_chunks.extend(chunks)
            pdf_metadata.extend([{"source": "pdf", "file": file_name}] * len(chunks))

"""Standardizing all the chunks together"""

standardized_chunks = []


for i, text in enumerate(pdf_chunks):
    standardized_chunks.append({
        'chunk_text': text,
        'source': 'pdf',
        'section': f'pdf_chunk_{i}'
    })

for i, text in enumerate(chunks_csv):
    standardized_chunks.append({
        'chunk_text': text,
        'source': 'csv',
        'section': f'csv_row_{i}'
    })

for i, text in enumerate(wiki_chunks):
    standardized_chunks.append({
        'chunk_text': text,
        'source': 'wikipedia',
        'section': f'wiki_para_{i}'
    })


standardized_chunks.extend(all_text_chunks_img)  # Already dictionary format

"""Embedding Chunks"""

from sentence_transformers import SentenceTransformer


embedder = SentenceTransformer('all-MiniLM-L6-v2')


texts = [chunk['chunk_text'] for chunk in standardized_chunks]
embeddings = embedder.encode(texts, show_progress_bar=True)

"""Creating Faiss Database"""

embedding_matrix = np.array(embeddings).astype('float32')

# Create FAISS index (L2 or cosine similarity)
index = faiss.IndexFlatL2(embedding_matrix.shape[1])  # Or use IndexFlatIP for cosine
index.add(embedding_matrix)

metadata = standardized_chunks

chunk_lookup = [chunk['chunk_text'] for chunk in standardized_chunks]
metadata_lookup = [
    {k: v for k, v in chunk.items() if k != 'chunk_text'}
    for chunk in standardized_chunks
]

"""Secret key loading"""

sec_key=userdata.get("HF_TOKEN")
sec_key=userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"]=sec_key

"""Loading LLM model"""

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3",
    load_in_8bit=True,
    device_map="auto"
)

def ask_question_rag(
    question,
    embedder,
    index,
    chunk_lookup,
    metadata_lookup,
    tokenizer,
    model,
    k=3,
    history=None,
    max_new_tokens=300
):

    query_vector = embedder.encode([question])

    #Retrieve top-k chunks from FAISS
    D, I = index.search(query_vector, k)
    retrieved = [(chunk_lookup[i], metadata_lookup[i]) for i in I[0] if i != -1]


    context = "\n\n".join([
        f"[{meta.get('source', 'unknown')} - {meta.get('section', 'no-section')}]\n{text.strip()}"
        for text, meta in retrieved
    ])

    #Build the prompt
    prompt = f"Use the following data to answer the question. Answer the question in paragraph, don't use options. Do not make assumptions, study clearly:\n\n{context}\n\n"



    if history:
        prompt += f"{history}\n"

    prompt += f"Question: {question}"


    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)


    final_answer = answer.replace(prompt, "").strip()

    return final_answer

"""Provding Questions for the model"""

questions = [
    "What is your understanding of the churn Rate?",
    "What are the major reasons for churn you infer from the CSV provided?"

]


results = []

for q in questions:
  response = ask_question_rag(
      question=q,
      embedder = embedder ,
      index=index,
      chunk_lookup=chunk_lookup,
      metadata_lookup=metadata_lookup,
      tokenizer=tokenizer,
      model=model,

  )
  results.append({"Question": q, "Generated Answer": response})





Final_op = pd.DataFrame(results)
Final_op.to_csv("Final_op.csv", index=False)
Final_op

generated_responses = [entry["Generated Answer"] for entry in results]
generated_responses = ' '.join(generated_responses)

standardized_chunks

generated_responses

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stop_words = set(ENGLISH_STOP_WORDS)

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    words = text.split()
    words = [word for word in words if word not in stop_words]

    return ' '.join(words)

"""Chunking Texts retrieved from OCR"""

Image_chunk_texts = [item['chunk_text'] for item in all_text_chunks_img]
Image_chunk_texts = ' '.join(Image_chunk_texts)

"""For finding cosine similarity All the data except csv is provided for reference chunk since reference chunks should not have numerical data as in out csv file..Reference chunk is a list of sentences of the input data which is used to find semantic similarity"""

import re

image_chunk_info = re.split(r'(?<=[.!?])\s+', Image_chunk_texts)
Reference_chunk = wiki_chunks + pdf_chunks + image_chunk_info

"""Finding Cosine similarity using various models"""

reference_chunks_cleaned = [preprocess(text) for text in Reference_chunk]

generated_text_cleaned = preprocess(generated_responses)

model = SentenceTransformer('all-MiniLM-L6-v2')


ref_embeddings = model.encode(reference_chunks_cleaned, convert_to_tensor=True)
gen_embeddings = model.encode([generated_text_cleaned], convert_to_tensor=True)

cos_sim_matrix = util.cos_sim(gen_embeddings, ref_embeddings)

for i, row in enumerate(cos_sim_matrix):
    best_score = row.max().item()
    best_match_idx = row.argmax().item()
    print(f"Generated chunk {i} best matches reference chunk {best_match_idx} with cosine similarity: {best_score:.4f}")

model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')



ref_embeddings = model.encode(reference_chunks_cleaned, convert_to_tensor=True)
gen_embeddings = model.encode([generated_text_cleaned], convert_to_tensor=True)


cos_sim_matrix = util.cos_sim(gen_embeddings, ref_embeddings)


for i, row in enumerate(cos_sim_matrix):
    best_score = row.max().item()
    best_match_idx = row.argmax().item()
    print(f"cosine similarity: {best_score:.4f}")

model = SentenceTransformer('all-mpnet-base-v2')


ref_embeddings = model.encode(reference_chunks_cleaned, convert_to_tensor=True)
gen_embeddings = model.encode([generated_text_cleaned], convert_to_tensor=True)

cos_sim_matrix = util.cos_sim(gen_embeddings, ref_embeddings)

for i, row in enumerate(cos_sim_matrix):
    best_score = row.max().item()
    best_match_idx = row.argmax().item()
    print(f"cosine similarity: {best_score:.4f}")

model = SentenceTransformer('intfloat/multilingual-e5-base')


ref_embeddings = model.encode(reference_chunks_cleaned, convert_to_tensor=True)
gen_embeddings = model.encode([generated_text_cleaned], convert_to_tensor=True)

cos_sim_matrix = util.cos_sim(gen_embeddings, ref_embeddings)

for i, row in enumerate(cos_sim_matrix):
    best_score = row.max().item()
    best_match_idx = row.argmax().item()
    print(f"cosine similarity: {best_score:.4f}")